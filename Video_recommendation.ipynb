{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"video recommendation\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file:///home/jatin/Desktop/Videos_Data_150819+6477.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accessControlId: long (nullable = true)\n",
      " |-- adminTags: string (nullable = true)\n",
      " |-- b4id: long (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- categoriesIds: string (nullable = true)\n",
      " |-- channel_image: string (nullable = true)\n",
      " |-- createdAt: long (nullable = true)\n",
      " |-- dataUrl: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- downloadUrl: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- endDate: long (nullable = true)\n",
      " |-- externalid: string (nullable = true)\n",
      " |-- flavorIds: string (nullable = true)\n",
      " |-- grabexternalid: string (nullable = true)\n",
      " |-- grabid: string (nullable = true)\n",
      " |-- grabproviderid: string (nullable = true)\n",
      " |-- grabvideoproductid: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- lastPlayedAt: long (nullable = true)\n",
      " |-- msDuration: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- partnerData: string (nullable = true)\n",
      " |-- partnerId: long (nullable = true)\n",
      " |-- plays: long (nullable = true)\n",
      " |-- pubdate: long (nullable = true)\n",
      " |-- rank: long (nullable = true)\n",
      " |-- searchText: string (nullable = true)\n",
      " |-- startDate: long (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- thumbnailUrl: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- updatedAt: long (nullable = true)\n",
      " |-- version: long (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      " |-- votes: long (nullable = true)\n",
      " |-- width: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevantFields = df.select(\"id\",\"name\",\"description\",\"tags\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Mark Halperin of Bloomberg News joins TODAY to talk about the growing rift in the Republican party now that Donald Trump is their likely presidential nominee. He predicts the GOP convention will be \"a big mess\" and questions whether Trump can win the White House without softening his tone.   Analyst Mark Halperin: \\'Very hard\\' for Donald Trump to win with current tone'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(map(unicode,relevantFields.first().asDict().values())).replace(\"0_kirvzs3g\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(relevantFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatAsDict(row):\n",
    "    key = str(row['id'])\n",
    "    value = \" \".join(map(unicode,row.asDict().values())).replace(key, \"\")\n",
    "    return (key,value)\n",
    "    \n",
    "# myRdd = relevantFields.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0_kirvzs3g',\n",
       " u'Mark Halperin of Bloomberg News joins TODAY to talk about the growing rift in the Republican party now that Donald Trump is their likely presidential nominee. He predicts the GOP convention will be \"a big mess\" and questions whether Trump can win the White House without softening his tone.   Analyst Mark Halperin: \\'Very hard\\' for Donald Trump to win with current tone')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatAsDict(relevantFields.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nameFromCsv = relevantFields.map(formatAsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nameFromCsv.saveAsTextFile(\"videoData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nameFromCsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputPath=\"/home/jatin/Documents/spark-notebook-0.6.2-scala-2.11.7-spark-1.6.0-hadoop-2.2.0-with-hive-with-parquet/notebooks/video-recommendation/videoMetaDataInCsvFormat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "englishStopWords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Reading file containing video meta data to make initial RDD\n",
    "# readFile = sc.textFile(inputPath).cache()\n",
    "# # readFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# readFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def formatInput(item):\n",
    "#      name = item.split(\",\")[0]\n",
    "#      return (name,item.replace(name+\",\",\"\"))\n",
    "\n",
    "# nameFromCsv = readFile.map(formatInput)\n",
    "# #                                name = item.split(\",\")(0)\n",
    "# #                               (name,item.replace(name+\",\",\"\"))\n",
    "# #                               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nameFromCsv.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reserveWords = sc.broadcast(englishStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def bagOfWords(item):\n",
    "    regexSpecialChars = \"\"\"[^a-zA-Z0-9\\s]\"\"\"\n",
    "    noSymbolsLine = re.sub(regexSpecialChars, \" \",item)\n",
    "    stopWords = reserveWords.value\n",
    "    listOfWords = noSymbolsLine.split()\n",
    "    nonEmptyWords = filter(lambda x :x != \"\",listOfWords)\n",
    "    removeReserveWords = []\n",
    "    for word in nonEmptyWords:\n",
    "        if word not in stopWords:\n",
    "            removeReserveWords.append(word.lower())\n",
    "    return removeReserveWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge', 'alaska', 'keeping', 'frontier', '1232', 'town', 'remote', 'all', 'takes', 'big', 'attraction', 'change', 'will', 'mccarthy', 'reality', 'tv', 'series', 'edge', 'alaska', 'mccarthy', 'alaska', 'frontier', 'town', 'grid', 'living', 'takeetna', 'alaska']\n"
     ]
    }
   ],
   "source": [
    "bow = bagOfWords(\"'Edge of Alaska': Keeping a Frontier 1232 Town  @ #Remote,All it takes is () *%* one big attraction to change everything. Will McCarthy be next?,\\\"reality tv series, edge of alaska, mccarthy alaska, frontier town, off grid living, takeetna alaska\\\"\")\n",
    "print bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandData = nameFromCsv.mapValues(bagOfWords).cache()\n",
    "# print cleandData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross Checking count of data after cleaning \n",
    "numDocs = cleandData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157296\n"
     ]
    }
   ],
   "source": [
    "print numDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# readFile.unpersist()\n",
    "from collections import Counter\n",
    "def calculateTermFreq(item):\n",
    "    return Counter(item).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "it = calculateTermFreq(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('town', 2), ('living', 1), ('keeping', 1), ('remote', 1), ('takes', 1), ('1232', 1), ('frontier', 2), ('tv', 1), ('big', 1), ('alaska', 4), ('mccarthy', 2), ('will', 1), ('attraction', 1), ('edge', 2), ('grid', 1), ('all', 1), ('series', 1), ('takeetna', 1), ('reality', 1), ('change', 1)]\n"
     ]
    }
   ],
   "source": [
    "print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "termDocumentFrequencies = cleandData.mapValues(calculateTermFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'halperin', 2), (u'tone', 2), (u'joins', 1), (u'nominee', 1), (u'win', 2), (u'hard', 1), (u'likely', 1), (u'predicts', 1), (u'trump', 3), (u'softening', 1), (u'house', 1), (u'mark', 2), (u'current', 1), (u'very', 1), (u'convention', 1), (u'party', 1), (u'white', 1), (u'gop', 1), (u'today', 1), (u'mess', 1), (u'big', 1), (u'questions', 1), (u'bloomberg', 1), (u'news', 1), (u'republican', 1), (u'analyst', 1), (u'presidential', 1), (u'he', 1), (u'rift', 1), (u'donald', 2), (u'growing', 1), (u'talk', 1)]\n"
     ]
    }
   ],
   "source": [
    "sampleItem = termDocumentFrequencies.take(1)[0][1]\n",
    "print sampleItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idDocs = termDocumentFrequencies.map(lambda x: x[0]).zipWithUniqueId().collectAsMap()#.toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(idDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# idDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docIds = dict((v,k) for k,v in idDocs.iteritems())\n",
    "# docIds.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0_kirvzs3g'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docIds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docVect = termDocumentFrequencies.flatMapValues(lambda x:x).values().reduceByKey(lambda x,y:x + y).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117863"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVect.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'redzepi'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVect.take(10)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabRdd = docVect.filter(lambda item: item[1] > 5 and len(item[0]) > 3).cache()\n",
    "corpusVocab = vocabRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35655\n"
     ]
    }
   ],
   "source": [
    "numOfFeatures = vocabRdd.count()\n",
    "vocabRdd.unpersist()\n",
    "print numOfFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO COMPUTE INVERSE DCOUMENT FREQUENCIES\n",
    "from math import log1p\n",
    "def inverseDocumentFrequencies(docFreqs, numDocs):\n",
    "    return map( lambda tup : (tup[0], log1p(float(numDocs) / tup[1])), docFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Computing inverse document frequencies 'idfs' from document frequencies\n",
    "idfs = inverseDocumentFrequencies(corpusVocab, numDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idfsMap = dict(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /* Broadcasting 'idfs' across nodes of cluster*/\n",
    "bidfs = sc.broadcast(idfsMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /* Collecting all the terms after filtering (terms, df) pairs*/\n",
    "termsIds = dict(enumerate(idfsMap.keys()))#.zipWithIndex()\n",
    "# print termsIds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(termsIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = dict((v,k) for k,v in termsIds.iteritems())\n",
    "termList = sc.broadcast(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVectWithId =  docVect.zipWithIndex().map(lambda x:(x[1],x[0])).cache()\n",
    "# // val pairs = docVectWithId.values.keys\n",
    "docVect.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pyspark.mllib.linalg import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "def createVector(item):\n",
    "    itemDict = dict(item)\n",
    "    allIdentifiers = termList.value#/* Locally obtaining broadcasted  values */\n",
    "    termInThisDocument = itemDict.keys()#/* Obtaining all terms from this document*/\n",
    "    sizeOfVector = len(allIdentifiers)#/* Computing number of terms(identifiers) across all the documents*/\n",
    "    tfidfMap = dict()#/* Computing a map of (identifier, tfidf) pairs from term-document\n",
    "#            (identifier, count) pairs and document-frequency (identifier, idfs) pair */\n",
    "    for term in termInThisDocument:\n",
    "        if term in allIdentifiers:\n",
    "            tfidfMap[allIdentifiers[term]]=itemDict[term]\n",
    "    return Vectors.sparse(numOfFeatures,tfidfMap)\n",
    "#     return tfidfMap\n",
    "tfidfVector = termDocumentFrequencies.mapValues(createVector)\n",
    "# import org.apache.spark.mllib.linalg.{Matrix, SingularValueDecomposition, Vectors, Matrices, Vector}\n",
    "# import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "# /* Constructing vector for metData of every video */\n",
    "\n",
    "# val tfidfVector: RDD[(String, Vector)] = termDocumentFrequencies.mapValues{termFreqPair =>\n",
    "#       val allIdentifiers = termList.value/* Locally obtaining broadcasted  values */\n",
    "#       //val docTotalTerms: Double = termFreqPair.values.sum + 0.0\n",
    "#       val termInThisDocument: List[String] = termFreqPair.keySet.toList/* Obtaining all terms from this document*/\n",
    "#       val sizeOfVector: Int = allIdentifiers.size/* Computing number of terms(identifiers) across all the documents*/\n",
    "#       var tfidfMap:Map[Int,Double] = Map()/* Computing a map of (identifier, tfidf) pairs from term-document\n",
    "#            (identifier, count) pairs and document-frequency (identifier, idfs) pair */\n",
    "#       for(term <- termInThisDocument if allIdentifiers.contains(term)) {\n",
    "#         tfidfMap += (allIdentifiers(term) -> termFreqPair(term)) /* TFIDF computation */\n",
    "#       }\n",
    "#       val tfidfSeq: Seq[(Int, Double)] = tfidfMap.toSeq/* Converting 'tfidfMap' map to a sequence */\n",
    "#       Vectors.sparse(sizeOfVector, tfidfSeq) /*Obtaining sparse vector from 'tfidfSeq' sequence and 'sizeOfVector'*/\n",
    "# }\n",
    "# tfidfVector.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidfVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(35655, {2318: 1.0, 2538: 2.0, 4191: 1.0, 4200: 1.0, 4888: 1.0, 6918: 1.0, 7240: 1.0, 8126: 1.0, 8433: 1.0, 9463: 1.0, 13932: 1.0, 15668: 1.0, 19574: 1.0, 22842: 1.0, 23445: 1.0, 23524: 1.0, 26162: 2.0, 26861: 1.0, 26869: 1.0, 28730: 1.0, 30829: 1.0, 31490: 1.0, 32040: 2.0, 32417: 1.0, 33838: 2.0, 34038: 3.0, 34176: 1.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createVector(sampleItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(35655, {2318: 1.0, 2538: 2.0, 4191: 1.0, 4200: 1.0, 4888: 1.0, 6918: 1.0, 7240: 1.0, 8126: 1.0, 8433: 1.0, 9463: 1.0, 13932: 1.0, 15668: 1.0, 19574: 1.0, 22842: 1.0, 23445: 1.0, 23524: 1.0, 26162: 2.0, 26861: 1.0, 26869: 1.0, 28730: 1.0, 30829: 1.0, 31490: 1.0, 32040: 2.0, 32417: 1.0, 33838: 2.0, 34038: 3.0, 34176: 1.0})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createVector(sampleItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVector.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "tfidfVector.count()\n",
    "docVect.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/* Constructing row matrix for terms and metaData of each video */\n",
    "mat = RowMatrix(tfidfVector.values())\n",
    "m = mat.numRows()# /* Number of rows in a matrix */\n",
    "n= mat.numCols()# /* Number of columns in a matrix */\n",
    "#/* Computing svd from the 'mat' to obtain matrices*/\n",
    "# svd = mat.computeSVD(30, computeU=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/33428589/pyspark-and-pca-how-can-i-extract-the-eigenvectors-of-this-pca-how-can-i-calcu/33500704#33500704\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSVD(row_matrix, k, computeU=False, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.common.JavaModelWrapper"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mat._java_matrix_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),), (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "# df = sqlContext.createDataFrame(data,[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = computeSVD(mat,30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.SVD"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = svd.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docSpaceRDD = U.rows.map(lambda x: x.toArray()[0]).zipWithUniqueId().sortByKey(False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docSpaceRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docSpace = docSpaceRDD#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 7252), (0.0, 55036), (0.0, 61700), (0.0, 61704), (0.0, 61712)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docSpaceRDD#.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docSpace[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 7252)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docSpace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = svd.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.DenseMatrix"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varr = V.toArray()\n",
    "type(varr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35655"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.numRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35655, 30)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = svd.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420.473586894,373.080269421,252.640112924,243.91454244,201.723838137,180.6726799,165.659494974,164.977221707,162.097112118,156.322604755,150.688622508,148.192225383,145.874218809,143.592145671,137.611575376,134.655238171,132.6661287,131.016917566,129.5086699,127.981286341,125.452312261,124.41057777,121.345643368,119.975577096,118.438668839,117.665289552,116.73072349,113.598003001,113.126344058,112.50725847]\n"
     ]
    }
   ],
   "source": [
    "print sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.DenseVector"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420.473586894,373.080269421,252.640112924,243.91454244,201.723838137,180.6726799,165.659494974,164.977221707,162.097112118,156.322604755,150.688622508,148.192225383,145.874218809,143.592145671,137.611575376,134.655238171,132.6661287,131.016917566,129.5086699,127.981286341,125.452312261,124.41057777,121.345643368,119.975577096,118.438668839,117.665289552,116.73072349,113.598003001,113.126344058,112.50725847]\n"
     ]
    }
   ],
   "source": [
    "print sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topTermsInTopConcepts(svd,numConcepts,numTerms,termIds):\n",
    "    v = svd.V #Matrix representing term space\n",
    "    topTerms = []\n",
    "    arr = v.toArray()\n",
    "    rows = v.numRows\n",
    "    for i in xrange(numConcepts):\n",
    "        termWeights = arr[:,i]#.zipWithIndex()\n",
    "        termList = list(enumerate(termWeights))\n",
    "        sortedArr = sorted(termList,key=lambda x: x[1],reverse=True)\n",
    "        topTerms.append(map(lambda x: (termIds[x[0]],x[1]),sortedArr[:numTerms]))              \n",
    "    return topTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findVidId(item):\n",
    "    docID = \"\"\n",
    "    idd = item[1]\n",
    "    if docIds.has_key(idd):\n",
    "        docID = docIds[idd]\n",
    "    else:\n",
    "        docID = None\n",
    "    return (docID,item[0])\n",
    "        \n",
    "def topDocsInTopConcepts(svd,numConcepts,numDocs,docIds):\n",
    "    u = svd.U\n",
    "    topDocs = []\n",
    "    for i in xrange(numConcepts):\n",
    "        docWeights =u.rows.map(lambda x: x.toArray()[i]).zipWithUniqueId().sortByKey(False).take(numDocs)\n",
    "        topDocs.append(map(findVidId,docWeights))\n",
    "    return topDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/* Extracts top terms from top most concepts */\n",
    "topConceptTerms = topTermsInTopConcepts(svd, 15, 15, termsIds)\n",
    "#/* Extracts top documents from top most concepts */\n",
    "topConceptDocs = topDocsInTopConcepts(svd, 15, 5, docIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zipper(x,y):\n",
    "    k = list(str(a[0]) for a in x)\n",
    "    v = list(str(a[0]) for a in y)\n",
    "    return (k,v)\n",
    "concepts = map(zipper,topConceptTerms, topConceptDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "# videoDataMap = nameFromCsv.cache()\n",
    "# videosMetaData = videoDataMap.map(lambda x: Row(Id=x[0],Info=x[1]))\n",
    "# schemaVideos = spark.createDataFrame(videosMetaData)\n",
    "# schemaVideos.createOrReplaceTempView(\"Videos\")\n",
    "# vidID = \"0_2umw3t4n\"\n",
    "# vidInQuotes = \"'%s'\" % vidID\n",
    "# query = \"SELECT Info FROM Videos WHERE Id = \" +vidInQuotes\n",
    "# print query\n",
    "# print \"SELECT Info FROM Videos WHERE Id = '0_2umw3t4n'\"\n",
    "# info = sqlContext.sql(query)\n",
    "# print info.collect()[0]['Info']\n",
    "\n",
    "# for k,v in concepts:\n",
    "#     print \"Concept Terms:\"\n",
    "#     print k\n",
    "#     print \"Concepts Documents:\"\n",
    "#     print v\n",
    "#     for a in v:\n",
    "#         if a != 'None':\n",
    "#             vidInQuotes = \"'%s'\" % a\n",
    "#             query = \"SELECT Info FROM Videos WHERE Id = \" +vidInQuotes\n",
    "#             info = sqlContext.sql(query).collect()\n",
    "# #             print type(info)\n",
    "#             if(len(info) == 1):\n",
    "#                 print info[0]['Info']\n",
    "#                 print\n",
    "#     print\n",
    "#     print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " m = DenseMatrix(2, 2, range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.DenseMatrix"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__UDT__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_convert_to_array',\n",
       " 'asML',\n",
       " 'isTransposed',\n",
       " 'numCols',\n",
       " 'numRows',\n",
       " 'toArray',\n",
       " 'toSparse',\n",
       " 'values']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.toArray().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def multiplyByDiagonalMatrix(mat: RowMatrix, diag: Vector): RowMatrix = {\n",
    "#     val sArr: Array[Double] = diag.toArray\n",
    "#     new RowMatrix(mat.rows.map(vec => {\n",
    "#       val vecArr: Array[Double] = vec.toArray\n",
    "#       val newArr: Array[Double] = (0 until vec.size).toArray.map(i => vecArr(i) * sArr(i))\n",
    "#       Vectors.dense(newArr)\n",
    "#     }))\n",
    "# }\n",
    "\n",
    "# def rowsNormalized(mat: RowMatrix): RowMatrix = {\n",
    "#     new RowMatrix(mat.rows.map(vec => {\n",
    "#       val length: Double = math.sqrt(vec.toArray.map(x => x * x).sum)\n",
    "#       Vectors.dense(vec.toArray.map(_ / length))\n",
    "#     }))\n",
    "# }\n",
    "import math\n",
    "sArr = svd.s.toArray()\n",
    "def multiply(vector):\n",
    "    vecArr = vector.toArray()\n",
    "    newArr = []\n",
    "    for i in xrange(vecArr.size):\n",
    "        newArr.append(vecArr[i]*sArr[i])\n",
    "    return newArr\n",
    "\n",
    "def normalizeRows(vector):\n",
    "    vecArr = vector.toArray()\n",
    "    length = math.sqrt(sum(map(lambda x: x*x,vecArr)))\n",
    "    output = map(lambda x: x/length,vecArr)\n",
    "    return output\n",
    "            \n",
    "def multiplyByDiagonalMatrix(mat,diag):\n",
    "#     sArr = diag.toArray()\n",
    "    return RowMatrix(mat.rows.map(multiply))\n",
    "    \n",
    "def rowsNormalized(mat):\n",
    "    return RowMatrix(mat.rows.map(normalizeRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "US = multiplyByDiagonalMatrix(svd.U, svd.s)\n",
    "normalizedUS = rowsNormalized(US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sArr = svd.s.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(normalizedUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.1938, 0.2231, 0.3539, 0.4166, -0.5493, -0.4202, 0.0078, 0.3901, 0.0726, -0.1069, -0.2168, -0.0156, 0.0182, -0.0479, -0.0176, -0.0021, 0.0184, -0.076, 0.3725, 0.7012, -0.538, -0.0264, -0.2791, -0.0837, 0.026, 0.4037, -0.4854, 0.3965, 0.9113, -0.2839])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US.rows.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.1027, 0.1182, 0.1875, 0.2207, -0.291, -0.2226, 0.0041, 0.2067, 0.0385, -0.0567, -0.1149, -0.0083, 0.0097, -0.0254, -0.0093, -0.0011, 0.0097, -0.0402, 0.1974, 0.3715, -0.285, -0.014, -0.1479, -0.0443, 0.0138, 0.2139, -0.2572, 0.2101, 0.4828, -0.1504])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizedUS.rows.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def printTopDocsForDoc(normalizedUS: RowMatrix, doc: String, idDocs: Map[String, Long],\n",
    "#       docIds: Map[Long, String], numTopDocs:Int) {\n",
    "#     try {\n",
    "#       val docID:Long = idDocs(doc)\n",
    "#       printIdWeights[Long](topDocsForDoc(normalizedUS, docID, numTopDocs), docIds)\n",
    "#     } catch {\n",
    "#       case ex: Exception => log.error(\"Exception doc not found {}\", ex)\n",
    "#       println(\"Term not found. Enter another term\")\n",
    "#     }\n",
    "#   }\n",
    "\n",
    "# def row(mat: RowMatrix, id: Long): Array[Double] = {\n",
    "#   mat.rows.zipWithUniqueId.map(_.swap).lookup(id).head.toArray\n",
    "# }\n",
    "\n",
    "# def topDocsForDoc(normalizedUS: RowMatrix, docId: Long, numTopDocs:Int): Seq[(Double, Long)] = {\n",
    "#     // Look up the row in US corresponding to the given doc ID.\n",
    "#     val docRowArr: Array[Double] = row(normalizedUS, docId)\n",
    "#     val docRowVec: Matrix = Matrices.dense(docRowArr.length, 1, docRowArr)\n",
    "\n",
    "\n",
    "#     // Compute scores against every doc\n",
    "#     val docScores: RowMatrix = normalizedUS.multiply(docRowVec)\n",
    "\n",
    "#     // Find the docs with the highest scores\n",
    "#     val allDocWeights: RDD[(Double, Long)] = docScores.rows.map(_.toArray(0)).zipWithUniqueId\n",
    "\n",
    "#     // Docs can end up with NaN score if their row in U is all zeros.  Filter these out.\n",
    "#     allDocWeights.filter(!_._1.isNaN).top(numTopDocs)\n",
    "#   }\n",
    "def row(mat,idd):\n",
    "    tuples = mat.rows.zipWithUniqueId().\n",
    "    \n",
    "\n",
    "def topDocsForDoc(normalizedUS, docId, numTopDocs):\n",
    "    docRowArr = row(normalizedUS, docId)\n",
    "    docRowVec = Matrices.dense(len(docRowArr), 1, docRowArr)\n",
    "    \n",
    "    \n",
    "def printTopDocsForDoc(normalizedUS, doc, idDocs, docIds, numTopDocs):\n",
    "    if idDocs.has_key(doc):\n",
    "        docID = idDocs(doc)\n",
    "        output = topDocsForDoc(normalizedUS, docID, numTopDocs)\n",
    "    else:\n",
    "        print \"Document not found. Enter another document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
