{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"video recommendation\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputPath=\"/home/jatin/Documents/spark-notebook-0.6.2-scala-2.11.7-spark-1.6.0-hadoop-2.2.0-with-hive-with-parquet/notebooks/video-recommendation/videoMetaDataInCsvFormat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "englishStopWords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59800"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading file containing video meta data to make initial RDD\n",
    "readFile = sc.textFile(inputPath).cache()\n",
    "readFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0_04xqwyas,\\'Edge of Alaska\\': Keeping a Frontier Town Remote,All it takes is one big attraction to change everything. Will McCarthy be next?,\"reality tv series, edge of alaska, mccarthy alaska, frontier town, off grid living, takeetna alaska\"',\n",
       " u'0_0m8u8g4c,\\'MythBusters\\': Shark Feeding Frenzy Defense,\"While testing the repelling abilities of \"\"essence of dead shark\"\", Jamie hopes his beret will serve as more than just a fashion statement.\",\"reality tv series, mythbusters, jamie hyneman, adam savage, urban legends, shark myths, feeding frenzy, mythbusters shark tests, shark armor\"',\n",
       " u'0_0pd5gia9,\\'MythBusters\\': A MythBusting Lightsaber Duel,Jamie and Adam put the myth of higher ground to the test in this epic lightsaber battle.,\"reality tv series, mythbusters, jamie hyneman, adam savage, urban legends, lightsaber dueling\"',\n",
       " u'0_0uwz3tkl,Behind the Scenes - Lucky Influencers: What Was Your Fashion First?,\"5 fashion industry influencers, Va$shtie Kola, Lily Kwong, Amirah Kassem, Christina Caradona and Ali Michaels, show off their favorite pieces sold on LuckyShops.com and share their first major purchases.\",\"luckymag.com, lucky magazine, lucky, luckyshops.com, fashion, shopping, style, va$htie kola, lily kwong, amirah kassem, christina caradona, ali michaels, season: season 1, series: behind the scenes\"',\n",
       " u'0_0v2yok9d,How Your Brain Experiences Virtual Reality,Virtual Reality may be the future but maybe not for women. Why does VR affect the sexes differently?,\"virtual reality, vr, sexism, neuroscience, gender, motion parallax, shape-from-shading, neurons, motion sickness, proprioception, oculus rift, perception, eyes, hormones, 3d, 3 dimensional, c-technology, dnews, education, science, discovery news, d news, age=14 15 16 17, julia wilde\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatInput(item):\n",
    "     name = item.split(\",\")[0]\n",
    "     return (name,item.replace(name+\",\",\"\"))\n",
    "\n",
    "nameFromCsv = readFile.map(formatInput)\n",
    "#                                name = item.split(\",\")(0)\n",
    "#                               (name,item.replace(name+\",\",\"\"))\n",
    "#                               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'0_04xqwyas',\n",
       "  u'\\'Edge of Alaska\\': Keeping a Frontier Town Remote,All it takes is one big attraction to change everything. Will McCarthy be next?,\"reality tv series, edge of alaska, mccarthy alaska, frontier town, off grid living, takeetna alaska\"'),\n",
       " (u'0_0m8u8g4c',\n",
       "  u'\\'MythBusters\\': Shark Feeding Frenzy Defense,\"While testing the repelling abilities of \"\"essence of dead shark\"\", Jamie hopes his beret will serve as more than just a fashion statement.\",\"reality tv series, mythbusters, jamie hyneman, adam savage, urban legends, shark myths, feeding frenzy, mythbusters shark tests, shark armor\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameFromCsv.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reserveWords = sc.broadcast(englishStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def bagOfWords(item):\n",
    "    regexSpecialChars = \"\"\"[^a-zA-Z0-9\\s]\"\"\"\n",
    "    noSymbolsLine = re.sub(regexSpecialChars, \" \",item)\n",
    "    stopWords = reserveWords.value\n",
    "    listOfWords = noSymbolsLine.split()\n",
    "    nonEmptyWords = filter(lambda x :x != \"\",listOfWords)\n",
    "    removeReserveWords = []\n",
    "    for word in nonEmptyWords:\n",
    "        if word not in stopWords:\n",
    "            removeReserveWords.append(word.lower())\n",
    "    return removeReserveWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge', 'alaska', 'keeping', 'frontier', '1232', 'town', 'remote', 'all', 'takes', 'big', 'attraction', 'change', 'will', 'mccarthy', 'reality', 'tv', 'series', 'edge', 'alaska', 'mccarthy', 'alaska', 'frontier', 'town', 'grid', 'living', 'takeetna', 'alaska']\n"
     ]
    }
   ],
   "source": [
    "bow = bagOfWords(\"'Edge of Alaska': Keeping a Frontier 1232 Town  @ #Remote,All it takes is () *%* one big attraction to change everything. Will McCarthy be next?,\\\"reality tv series, edge of alaska, mccarthy alaska, frontier town, off grid living, takeetna alaska\\\"\")\n",
    "print bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[8] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "cleandData = nameFromCsv.mapValues(bagOfWords).cache()\n",
    "print cleandData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross Checking count of data after cleaning \n",
    "numDocs = cleandData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59800\n"
     ]
    }
   ],
   "source": [
    "print numDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readFile.unpersist()\n",
    "from collections import Counter\n",
    "def calculateTermFreq(item):\n",
    "    return Counter(item).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "it = calculateTermFreq(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('town', 2), ('living', 1), ('keeping', 1), ('remote', 1), ('takes', 1), ('1232', 1), ('frontier', 2), ('tv', 1), ('big', 1), ('alaska', 4), ('mccarthy', 2), ('will', 1), ('attraction', 1), ('edge', 2), ('grid', 1), ('all', 1), ('series', 1), ('takeetna', 1), ('reality', 1), ('change', 1)]\n"
     ]
    }
   ],
   "source": [
    "print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "termDocumentFrequencies = cleandData.mapValues(calculateTermFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'town', 2), (u'living', 1), (u'keeping', 1), (u'remote', 1), (u'takes', 1), (u'frontier', 2), (u'tv', 1), (u'big', 1), (u'alaska', 4), (u'mccarthy', 2), (u'will', 1), (u'attraction', 1), (u'edge', 2), (u'grid', 1), (u'all', 1), (u'series', 1), (u'takeetna', 1), (u'reality', 1), (u'change', 1)]\n"
     ]
    }
   ],
   "source": [
    "sampleItem = termDocumentFrequencies.take(1)[0][1]\n",
    "print sampleItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idDocs = termDocumentFrequencies.map(lambda x: x[0]).zipWithUniqueId().collectAsMap()#.toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(idDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docIds = dict((v,k) for k,v in idDocs.iteritems())\n",
    "# docIds.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0_04xqwyas'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docIds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docVect = termDocumentFrequencies.flatMapValues(lambda x:x).values().reduceByKey(lambda x,y:x + y).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34629"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVect.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'abenteuerliche'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVect.take(10)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabRdd = docVect.filter(lambda item: item[1] > 5 and len(item[0]) > 3).cache()\n",
    "corpusVocab = vocabRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9834\n"
     ]
    }
   ],
   "source": [
    "numOfFeatures = vocabRdd.count()\n",
    "vocabRdd.unpersist()\n",
    "print numOfFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO COMPUTE INVERSE DCOUMENT FREQUENCIES\n",
    "from math import log1p\n",
    "def inverseDocumentFrequencies(docFreqs, numDocs):\n",
    "    return map( lambda tup : (tup[0], log1p(float(numDocs) / tup[1])), docFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Computing inverse document frequencies 'idfs' from document frequencies\n",
    "idfs = inverseDocumentFrequencies(corpusVocab, numDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idfsMap = dict(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /* Broadcasting 'idfs' across nodes of cluster*/\n",
    "bidfs = sc.broadcast(idfsMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /* Collecting all the terms after filtering (terms, df) pairs*/\n",
    "termsIds = dict(enumerate(idfsMap.keys()))#.zipWithIndex()\n",
    "# print termsIds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(termsIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = dict((v,k) for k,v in termsIds.iteritems())\n",
    "termList = sc.broadcast(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVectWithId =  docVect.zipWithIndex().map(lambda x:(x[1],x[0])).cache()\n",
    "# // val pairs = docVectWithId.values.keys\n",
    "docVect.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pyspark.mllib.linalg import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "def createVector(item):\n",
    "    itemDict = dict(item)\n",
    "    allIdentifiers = termList.value#/* Locally obtaining broadcasted  values */\n",
    "    termInThisDocument = itemDict.keys()#/* Obtaining all terms from this document*/\n",
    "    sizeOfVector = len(allIdentifiers)#/* Computing number of terms(identifiers) across all the documents*/\n",
    "    tfidfMap = dict()#/* Computing a map of (identifier, tfidf) pairs from term-document\n",
    "#            (identifier, count) pairs and document-frequency (identifier, idfs) pair */\n",
    "    for term in termInThisDocument:\n",
    "        if term in allIdentifiers:\n",
    "            tfidfMap[allIdentifiers[term]]=itemDict[term]\n",
    "    return Vectors.sparse(numOfFeatures,tfidfMap)\n",
    "#     return tfidfMap\n",
    "tfidfVector = termDocumentFrequencies.mapValues(createVector)\n",
    "# import org.apache.spark.mllib.linalg.{Matrix, SingularValueDecomposition, Vectors, Matrices, Vector}\n",
    "# import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "# /* Constructing vector for metData of every video */\n",
    "\n",
    "# val tfidfVector: RDD[(String, Vector)] = termDocumentFrequencies.mapValues{termFreqPair =>\n",
    "#       val allIdentifiers = termList.value/* Locally obtaining broadcasted  values */\n",
    "#       //val docTotalTerms: Double = termFreqPair.values.sum + 0.0\n",
    "#       val termInThisDocument: List[String] = termFreqPair.keySet.toList/* Obtaining all terms from this document*/\n",
    "#       val sizeOfVector: Int = allIdentifiers.size/* Computing number of terms(identifiers) across all the documents*/\n",
    "#       var tfidfMap:Map[Int,Double] = Map()/* Computing a map of (identifier, tfidf) pairs from term-document\n",
    "#            (identifier, count) pairs and document-frequency (identifier, idfs) pair */\n",
    "#       for(term <- termInThisDocument if allIdentifiers.contains(term)) {\n",
    "#         tfidfMap += (allIdentifiers(term) -> termFreqPair(term)) /* TFIDF computation */\n",
    "#       }\n",
    "#       val tfidfSeq: Seq[(Int, Double)] = tfidfMap.toSeq/* Converting 'tfidfMap' map to a sequence */\n",
    "#       Vectors.sparse(sizeOfVector, tfidfSeq) /*Obtaining sparse vector from 'tfidfSeq' sequence and 'sizeOfVector'*/\n",
    "# }\n",
    "# tfidfVector.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidfVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(9834, {184: 1.0, 765: 1.0, 1299: 1.0, 2943: 1.0, 3044: 1.0, 3696: 1.0, 5061: 2.0, 5719: 1.0, 6034: 1.0, 6154: 1.0, 6259: 2.0, 6828: 4.0, 9086: 2.0, 9439: 2.0, 9633: 1.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createVector(sampleItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(9834, {184: 1.0, 765: 1.0, 1299: 1.0, 2943: 1.0, 3044: 1.0, 3696: 1.0, 5061: 2.0, 5719: 1.0, 6034: 1.0, 6154: 1.0, 6259: 2.0, 6828: 4.0, 9086: 2.0, 9439: 2.0, 9633: 1.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createVector(sampleItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVector.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "tfidfVector.count()\n",
    "docVect.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/* Constructing row matrix for terms and metaData of each video */\n",
    "mat = RowMatrix(tfidfVector.values())\n",
    "m = mat.numRows()# /* Number of rows in a matrix */\n",
    "n= mat.numCols()# /* Number of columns in a matrix */\n",
    "#/* Computing svd from the 'mat' to obtain matrices*/\n",
    "# svd = mat.computeSVD(30, computeU=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samMat = tfidfVector.values().map(lambda item: item.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samMat.saveAsTextFile(\"matrix\")\n",
    "type(samMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sm = samMat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(samMat.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(9, 6) + 1j*np.random.randn(9, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# U, s, V = np.linalg.svd(samMat.collect(), full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U.shape, V.shape, s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = np.zeros((9, 6), dtype=complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S[:6, :6] = np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(a, np.dot(U, np.dot(S, V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(mat, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy, scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparsesvd import sparsesvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = numpy.random.rand(200, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smat = scipy.sparse.csc_matrix(samMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(smat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ut, s, vt = sparsesvd(smat, 100) # do SVD, asking for 100 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert numpy.allclose(mat, numpy.dot(ut.T, numpy.dot(numpy.diag(s), vt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import genism\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/33428589/pyspark-and-pca-how-can-i-extract-the-eigenvectors-of-this-pca-how-can-i-calcu/33500704#33500704\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSVD(row_matrix, k, computeU=False, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),), (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "# df = sqlContext.createDataFrame(data,[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = computeSVD(mat,30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = svd.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = svd.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.DenseMatrix"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "varr = V.toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9834"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.numRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = svd.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.DenseVector"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[261.340500239,157.326732017,136.585630796,121.555643588,120.213496074,116.853580174,116.185708358,114.679599252,111.598589588,110.232222097,109.363381113,107.785967392,107.209801091,106.356741268,105.83567632,105.551323328,104.611021707,103.9403295,103.289137349,102.648107231,101.871406226,101.674952761,101.120055175,100.336426178,99.4333697057,99.4116321977,98.0105621693,97.2515265761,97.0227908872,96.1518359425]\n"
     ]
    }
   ],
   "source": [
    "print sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topTermsInTopConcepts(svd,numConcepts,numTerms,termIds):\n",
    "    v = svd.V #Matrix representing term space\n",
    "    topTerms = []\n",
    "    arr = v.toArray\n",
    "    rows = v.numRows\n",
    "    for i in xrange(numConcepts):\n",
    "        offs = i*rows\n",
    "        termWeights = arr[offs:offs+rows].zipWithIndex()\n",
    "        sortedArr = list(enumerate(termWeights))\n",
    "        topTerms = \n",
    "# import scala.collection.mutable.ArrayBuffer\n",
    "\n",
    "# /* FUNCTION TO COMPUTE TOP TERMS IN TOP CONCEPTS*/\n",
    "#   def topTermsInTopConcepts(svd: SingularValueDecomposition[RowMatrix, Matrix], numConcepts: Int,\n",
    "#       numTerms: Int, termIds: Map[Int, String]): Seq[Seq[(String, Double)]] = {\n",
    "#     val v: Matrix = svd.V /* Matrix representing term space*/\n",
    "#     val topTerms: ArrayBuffer[Seq[(String, Double)]] = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "#     val arr: Array[Double] = v.toArray\n",
    "#     for (i <- 0 until numConcepts) {\n",
    "#       val offs: Int = i * v.numRows\n",
    "#       val termWeights: Array[(Double, Int)] = arr.slice(offs, offs + v.numRows).zipWithIndex /* Picking each column of the matrix 'v'*/\n",
    "#       val sorted: Array[(Double, Int)] = termWeights.sortBy(-_._1)\n",
    "#       topTerms += sorted.take(numTerms).map{case (score, id) => (termIds(id), score)} /* Associating scores with\n",
    "#       corresponding terms using termIds*/\n",
    "#     }\n",
    "#     topTerms\n",
    "#   }\n",
    "\n",
    "#   /* FUNCTION TO COMPUTE TOP DOCUMENTS IN TOP CONCEPTS*/\n",
    "#   def topDocsInTopConcepts(svd: SingularValueDecomposition[RowMatrix, Matrix], numConcepts: Int,\n",
    "#       numDocs: Int, docIds: Map[Long, String]): Seq[Seq[(String, Double)]] = {\n",
    "#     val u: RowMatrix = svd.U /* Matrix representing document space*/\n",
    "#     val topDocs: ArrayBuffer[Seq[(String, Double)]] = new ArrayBuffer[Seq[(String, Double)]]()\n",
    "#     for (i <- 0 until numConcepts) {\n",
    "#       val docWeights: RDD[(Double, Long)] = u.rows.map(_.toArray(i)).zipWithUniqueId  /* Picking each row of the row matrix 'u'*/\n",
    "#       topDocs += docWeights.top(numDocs).map{case (score, id) => \n",
    "#                                              var docID = \"\"\n",
    "#                                              try{\n",
    "#                                                docID = docIds(id)\n",
    "#                                              }\n",
    "#                                              catch {\n",
    "#                                                case ex: Exception => log.error(\"Doc ID does not Exist\", ex)\n",
    "#                                                docID = \"\"\n",
    "#                                              }  \n",
    "#                                              (docID, score)} /* Associating scores with\n",
    "#       corresponding documents using docIds */\n",
    "#     }\n",
    "#     topDocs\n",
    "#   }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
