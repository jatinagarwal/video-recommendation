{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"video recommendation\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file:///home/jatin/Desktop/Videos_Data_150819+6477.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- accessControlId: long (nullable = true)\n",
      " |-- adminTags: string (nullable = true)\n",
      " |-- b4id: long (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- categoriesIds: string (nullable = true)\n",
      " |-- channel_image: string (nullable = true)\n",
      " |-- createdAt: long (nullable = true)\n",
      " |-- dataUrl: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- downloadUrl: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- endDate: long (nullable = true)\n",
      " |-- externalid: string (nullable = true)\n",
      " |-- flavorIds: string (nullable = true)\n",
      " |-- grabexternalid: string (nullable = true)\n",
      " |-- grabid: string (nullable = true)\n",
      " |-- grabproviderid: string (nullable = true)\n",
      " |-- grabvideoproductid: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- lastPlayedAt: long (nullable = true)\n",
      " |-- msDuration: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- partnerData: string (nullable = true)\n",
      " |-- partnerId: long (nullable = true)\n",
      " |-- plays: long (nullable = true)\n",
      " |-- pubdate: long (nullable = true)\n",
      " |-- rank: long (nullable = true)\n",
      " |-- searchText: string (nullable = true)\n",
      " |-- startDate: long (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- thumbnailUrl: string (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- updatedAt: long (nullable = true)\n",
      " |-- version: long (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      " |-- votes: long (nullable = true)\n",
      " |-- width: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevantFields = df.select(\"id\",\"name\",\"description\",\"tags\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Mark Halperin of Bloomberg News joins TODAY to talk about the growing rift in the Republican party now that Donald Trump is their likely presidential nominee. He predicts the GOP convention will be \"a big mess\" and questions whether Trump can win the White House without softening his tone.   Analyst Mark Halperin: \\'Very hard\\' for Donald Trump to win with current tone'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(map(unicode,relevantFields.first().asDict().values())).replace(\"0_kirvzs3g\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(relevantFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatAsDict(row):\n",
    "    key = str(row['id'])\n",
    "    value = \" \".join(map(unicode,row.asDict().values())).replace(key, \"\")\n",
    "    return (key,value)\n",
    "    \n",
    "# myRdd = relevantFields.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0_kirvzs3g',\n",
       " u'Mark Halperin of Bloomberg News joins TODAY to talk about the growing rift in the Republican party now that Donald Trump is their likely presidential nominee. He predicts the GOP convention will be \"a big mess\" and questions whether Trump can win the White House without softening his tone.   Analyst Mark Halperin: \\'Very hard\\' for Donald Trump to win with current tone')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatAsDict(relevantFields.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nameFromCsv = relevantFields.map(formatAsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nameFromCsv.saveAsTextFile(\"videoData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nameFromCsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputPath=\"/home/jatin/Documents/spark-notebook-0.6.2-scala-2.11.7-spark-1.6.0-hadoop-2.2.0-with-hive-with-parquet/notebooks/video-recommendation/videoMetaDataInCsvFormat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "englishStopWords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Reading file containing video meta data to make initial RDD\n",
    "# readFile = sc.textFile(inputPath).cache()\n",
    "# # readFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# readFile.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def formatInput(item):\n",
    "#      name = item.split(\",\")[0]\n",
    "#      return (name,item.replace(name+\",\",\"\"))\n",
    "\n",
    "# nameFromCsv = readFile.map(formatInput)\n",
    "# #                                name = item.split(\",\")(0)\n",
    "# #                               (name,item.replace(name+\",\",\"\"))\n",
    "# #                               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nameFromCsv.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reserveWords = sc.broadcast(englishStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def bagOfWords(item):\n",
    "    regexSpecialChars = \"\"\"[^a-zA-Z0-9\\s]\"\"\"\n",
    "    noSymbolsLine = re.sub(regexSpecialChars, \" \",item)\n",
    "    stopWords = reserveWords.value\n",
    "    listOfWords = noSymbolsLine.split()\n",
    "    nonEmptyWords = filter(lambda x :x != \"\",listOfWords)\n",
    "    removeReserveWords = []\n",
    "    for word in nonEmptyWords:\n",
    "        if word not in stopWords:\n",
    "            removeReserveWords.append(word.lower())\n",
    "    return removeReserveWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge', 'alaska', 'keeping', 'frontier', '1232', 'town', 'remote', 'all', 'takes', 'big', 'attraction', 'change', 'will', 'mccarthy', 'reality', 'tv', 'series', 'edge', 'alaska', 'mccarthy', 'alaska', 'frontier', 'town', 'grid', 'living', 'takeetna', 'alaska']\n"
     ]
    }
   ],
   "source": [
    "bow = bagOfWords(\"'Edge of Alaska': Keeping a Frontier 1232 Town  @ #Remote,All it takes is () *%* one big attraction to change everything. Will McCarthy be next?,\\\"reality tv series, edge of alaska, mccarthy alaska, frontier town, off grid living, takeetna alaska\\\"\")\n",
    "print bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandData = nameFromCsv.mapValues(bagOfWords).cache()\n",
    "# print cleandData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross Checking count of data after cleaning \n",
    "numDocs = cleandData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157296\n"
     ]
    }
   ],
   "source": [
    "print numDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# readFile.unpersist()\n",
    "from collections import Counter\n",
    "def calculateTermFreq(item):\n",
    "    return Counter(item).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "it = calculateTermFreq(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('town', 2), ('living', 1), ('keeping', 1), ('remote', 1), ('takes', 1), ('1232', 1), ('frontier', 2), ('tv', 1), ('big', 1), ('alaska', 4), ('mccarthy', 2), ('will', 1), ('attraction', 1), ('edge', 2), ('grid', 1), ('all', 1), ('series', 1), ('takeetna', 1), ('reality', 1), ('change', 1)]\n"
     ]
    }
   ],
   "source": [
    "print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "termDocumentFrequencies = cleandData.mapValues(calculateTermFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'halperin', 2), (u'tone', 2), (u'joins', 1), (u'nominee', 1), (u'win', 2), (u'hard', 1), (u'likely', 1), (u'predicts', 1), (u'trump', 3), (u'softening', 1), (u'house', 1), (u'mark', 2), (u'current', 1), (u'very', 1), (u'convention', 1), (u'party', 1), (u'white', 1), (u'gop', 1), (u'today', 1), (u'mess', 1), (u'big', 1), (u'questions', 1), (u'bloomberg', 1), (u'news', 1), (u'republican', 1), (u'analyst', 1), (u'presidential', 1), (u'he', 1), (u'rift', 1), (u'donald', 2), (u'growing', 1), (u'talk', 1)]\n"
     ]
    }
   ],
   "source": [
    "sampleItem = termDocumentFrequencies.take(1)[0][1]\n",
    "print sampleItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idDocs = termDocumentFrequencies.map(lambda x: x[0]).zipWithUniqueId().collectAsMap()#.toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(idDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# idDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docIds = dict((v,k) for k,v in idDocs.iteritems())\n",
    "# docIds.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0_kirvzs3g'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docIds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docVect = termDocumentFrequencies.flatMapValues(lambda x:x).values().reduceByKey(lambda x,y:x + y).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117863"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVect.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'redzepi'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVect.take(10)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabRdd = docVect.filter(lambda item: item[1] > 5 and len(item[0]) > 3).cache()\n",
    "corpusVocab = vocabRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35655\n"
     ]
    }
   ],
   "source": [
    "numOfFeatures = vocabRdd.count()\n",
    "vocabRdd.unpersist()\n",
    "print numOfFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION TO COMPUTE INVERSE DCOUMENT FREQUENCIES\n",
    "from math import log1p\n",
    "def inverseDocumentFrequencies(docFreqs, numDocs):\n",
    "    return map( lambda tup : (tup[0], log1p(float(numDocs) / tup[1])), docFreqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Computing inverse document frequencies 'idfs' from document frequencies\n",
    "idfs = inverseDocumentFrequencies(corpusVocab, numDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idfsMap = dict(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /* Broadcasting 'idfs' across nodes of cluster*/\n",
    "bidfs = sc.broadcast(idfsMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# /* Collecting all the terms after filtering (terms, df) pairs*/\n",
    "termsIds = dict(enumerate(idfsMap.keys()))#.zipWithIndex()\n",
    "# print termsIds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(termsIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = dict((v,k) for k,v in termsIds.iteritems())\n",
    "termList = sc.broadcast(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVectWithId =  docVect.zipWithIndex().map(lambda x:(x[1],x[0])).cache()\n",
    "# // val pairs = docVectWithId.values.keys\n",
    "docVect.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pyspark.mllib.linalg import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "def createVector(item):\n",
    "    itemDict = dict(item)\n",
    "    allIdentifiers = termList.value#/* Locally obtaining broadcasted  values */\n",
    "    termInThisDocument = itemDict.keys()#/* Obtaining all terms from this document*/\n",
    "    sizeOfVector = len(allIdentifiers)#/* Computing number of terms(identifiers) across all the documents*/\n",
    "    tfidfMap = dict()#/* Computing a map of (identifier, tfidf) pairs from term-document\n",
    "#            (identifier, count) pairs and document-frequency (identifier, idfs) pair */\n",
    "    for term in termInThisDocument:\n",
    "        if term in allIdentifiers:\n",
    "            tfidfMap[allIdentifiers[term]]=itemDict[term]\n",
    "    return Vectors.sparse(numOfFeatures,tfidfMap)\n",
    "#     return tfidfMap\n",
    "tfidfVector = termDocumentFrequencies.mapValues(createVector)\n",
    "# import org.apache.spark.mllib.linalg.{Matrix, SingularValueDecomposition, Vectors, Matrices, Vector}\n",
    "# import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "# /* Constructing vector for metData of every video */\n",
    "\n",
    "# val tfidfVector: RDD[(String, Vector)] = termDocumentFrequencies.mapValues{termFreqPair =>\n",
    "#       val allIdentifiers = termList.value/* Locally obtaining broadcasted  values */\n",
    "#       //val docTotalTerms: Double = termFreqPair.values.sum + 0.0\n",
    "#       val termInThisDocument: List[String] = termFreqPair.keySet.toList/* Obtaining all terms from this document*/\n",
    "#       val sizeOfVector: Int = allIdentifiers.size/* Computing number of terms(identifiers) across all the documents*/\n",
    "#       var tfidfMap:Map[Int,Double] = Map()/* Computing a map of (identifier, tfidf) pairs from term-document\n",
    "#            (identifier, count) pairs and document-frequency (identifier, idfs) pair */\n",
    "#       for(term <- termInThisDocument if allIdentifiers.contains(term)) {\n",
    "#         tfidfMap += (allIdentifiers(term) -> termFreqPair(term)) /* TFIDF computation */\n",
    "#       }\n",
    "#       val tfidfSeq: Seq[(Int, Double)] = tfidfMap.toSeq/* Converting 'tfidfMap' map to a sequence */\n",
    "#       Vectors.sparse(sizeOfVector, tfidfSeq) /*Obtaining sparse vector from 'tfidfSeq' sequence and 'sizeOfVector'*/\n",
    "# }\n",
    "# tfidfVector.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidfVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(35655, {2318: 1.0, 2538: 2.0, 4191: 1.0, 4200: 1.0, 4888: 1.0, 6918: 1.0, 7240: 1.0, 8126: 1.0, 8433: 1.0, 9463: 1.0, 13932: 1.0, 15668: 1.0, 19574: 1.0, 22842: 1.0, 23445: 1.0, 23524: 1.0, 26162: 2.0, 26861: 1.0, 26869: 1.0, 28730: 1.0, 30829: 1.0, 31490: 1.0, 32040: 2.0, 32417: 1.0, 33838: 2.0, 34038: 3.0, 34176: 1.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createVector(sampleItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(35655, {2318: 1.0, 2538: 2.0, 4191: 1.0, 4200: 1.0, 4888: 1.0, 6918: 1.0, 7240: 1.0, 8126: 1.0, 8433: 1.0, 9463: 1.0, 13932: 1.0, 15668: 1.0, 19574: 1.0, 22842: 1.0, 23445: 1.0, 23524: 1.0, 26162: 2.0, 26861: 1.0, 26869: 1.0, 28730: 1.0, 30829: 1.0, 31490: 1.0, 32040: 2.0, 32417: 1.0, 33838: 2.0, 34038: 3.0, 34176: 1.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createVector(sampleItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[29] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVector.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "tfidfVector.count()\n",
    "docVect.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/* Constructing row matrix for terms and metaData of each video */\n",
    "mat = RowMatrix(tfidfVector.values())\n",
    "m = mat.numRows()# /* Number of rows in a matrix */\n",
    "n= mat.numCols()# /* Number of columns in a matrix */\n",
    "#/* Computing svd from the 'mat' to obtain matrices*/\n",
    "# svd = mat.computeSVD(30, computeU=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/33428589/pyspark-and-pca-how-can-i-extract-the-eigenvectors-of-this-pca-how-can-i-calcu/33500704#33500704\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeSVD(row_matrix, k, computeU=False, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.common.JavaModelWrapper"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mat._java_matrix_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),), (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "# df = sqlContext.createDataFrame(data,[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = computeSVD(mat,30,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.SVD"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = svd.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docSpaceRDD = U.rows.map(lambda x: x.toArray()[0]).zipWithUniqueId().sortByKey(False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docSpaceRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docSpace = docSpaceRDD#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 7252), (0.0, 55036), (0.0, 61700), (0.0, 61704), (0.0, 61712)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docSpaceRDD#.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docSpace[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 7252)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docSpace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.distributed.RowMatrix"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = svd.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.DenseMatrix"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varr = V.toArray()\n",
    "type(varr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35655"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.numRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35655, 30)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = svd.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420.473586894,373.080269421,252.640112924,243.91454244,201.723838137,180.6726799,165.659494974,164.977221707,162.097112118,156.322604755,150.688622508,148.192225383,145.874218809,143.592145671,137.611575376,134.655238171,132.6661287,131.016917566,129.5086699,127.981286341,125.452312261,124.41057777,121.345643368,119.975577096,118.438668839,117.665289552,116.73072349,113.598003001,113.126344058,112.50725847]\n"
     ]
    }
   ],
   "source": [
    "print sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.linalg.DenseVector"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420.473586894,373.080269421,252.640112924,243.91454244,201.723838137,180.6726799,165.659494974,164.977221707,162.097112118,156.322604755,150.688622508,148.192225383,145.874218809,143.592145671,137.611575376,134.655238171,132.6661287,131.016917566,129.5086699,127.981286341,125.452312261,124.41057777,121.345643368,119.975577096,118.438668839,117.665289552,116.73072349,113.598003001,113.126344058,112.50725847]\n"
     ]
    }
   ],
   "source": [
    "print sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topTermsInTopConcepts(svd,numConcepts,numTerms,termIds):\n",
    "    v = svd.V #Matrix representing term space\n",
    "    topTerms = []\n",
    "    arr = v.toArray()\n",
    "    rows = v.numRows\n",
    "    for i in xrange(numConcepts):\n",
    "        termWeights = arr[:,i]#.zipWithIndex()\n",
    "        termList = list(enumerate(termWeights))\n",
    "        sortedArr = sorted(termList,key=lambda x: x[1],reverse=True)\n",
    "        topTerms.append(map(lambda x: (termIds[x[0]],x[1]),sortedArr[:numTerms]))              \n",
    "    return topTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findVidId(item):\n",
    "    docID = \"\"\n",
    "    idd = item[1]\n",
    "    if docIds.has_key(idd):\n",
    "        docID = docIds[idd]\n",
    "    else:\n",
    "        docID = None\n",
    "    return (docID,item[0])\n",
    "        \n",
    "def topDocsInTopConcepts(svd,numConcepts,numDocs,docIds):\n",
    "    u = svd.U\n",
    "    topDocs = []\n",
    "    for i in xrange(numConcepts):\n",
    "        docWeights =u.rows.map(lambda x: x.toArray()[i]).zipWithUniqueId().sortByKey(False).take(numDocs)\n",
    "        topDocs.append(map(findVidId,docWeights))\n",
    "    return topDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/* Extracts top terms from top most concepts */\n",
    "topConceptTerms = topTermsInTopConcepts(svd, 15, 15, termsIds)\n",
    "#/* Extracts top documents from top most concepts */\n",
    "topConceptDocs = topDocsInTopConcepts(svd, 15, 5, docIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zipper(x,y):\n",
    "    k = list(str(a[0]) for a in x)\n",
    "    v = list(str(a[0]) for a in y)\n",
    "    return (k,v)\n",
    "concepts = map(zipper,topConceptTerms, topConceptDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Info FROM Videos WHERE Id = '0_2umw3t4n'\n",
      "SELECT Info FROM Videos WHERE Id = '0_2umw3t4n'\n",
      "Runway, backstage, and front row footage from the Milan show. Watch the Gucci Spring 2016 ready-to-wear fashion show from Vogue.com. Want more? Visit Vogue.com for more runway shows, fashion trends, shopping guides, and news about models and designers. vogue, vogue magazine, fashion, runway shows, fashion week, milan, milan fashion week, gucci, spring 2016, gucci spring 2016, designers, runway, alessandro michele, backstage, fashion week backstage, behind the scenes, fashion week behind the scenes, season: 2016, series: vogue fashion week  Vogue Fashion Week - Gucci Spring 2016 Ready-to-Wear\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "videoDataMap = nameFromCsv.cache()\n",
    "videosMetaData = videoDataMap.map(lambda x: Row(Id=x[0],Info=x[1]))\n",
    "schemaVideos = spark.createDataFrame(videosMetaData)\n",
    "schemaVideos.createOrReplaceTempView(\"Videos\")\n",
    "vidID = \"0_2umw3t4n\"\n",
    "vidInQuotes = \"'%s'\" % vidID\n",
    "query = \"SELECT Info FROM Videos WHERE Id = \" +vidInQuotes\n",
    "print query\n",
    "print \"SELECT Info FROM Videos WHERE Id = '0_2umw3t4n'\"\n",
    "info = sqlContext.sql(query)\n",
    "print info.collect()[0]['Info']\n",
    "\n",
    "def docMeaning(vidId):\n",
    "    if vidId != 'None':\n",
    "        vidInQuotes = \"'%s'\" % vidId\n",
    "        query = \"SELECT Info FROM Videos WHERE Id = \" +vidInQuotes\n",
    "        info = sqlContext.sql(query).collect()\n",
    "#             print type(info)\n",
    "        if(len(info) == 1):\n",
    "            print vidId\n",
    "            print info[0]['Info']\n",
    "            print\n",
    "            \n",
    "# for k,v in concepts:\n",
    "#     print \"Concept Terms:\"\n",
    "#     print k\n",
    "#     print \"Concepts Documents:\"\n",
    "#     print v\n",
    "#     for a in v:\n",
    "#         docMeaning(a)\n",
    "#     print\n",
    "#     print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_2umw3t4n\n",
      "Runway, backstage, and front row footage from the Milan show. Watch the Gucci Spring 2016 ready-to-wear fashion show from Vogue.com. Want more? Visit Vogue.com for more runway shows, fashion trends, shopping guides, and news about models and designers. vogue, vogue magazine, fashion, runway shows, fashion week, milan, milan fashion week, gucci, spring 2016, gucci spring 2016, designers, runway, alessandro michele, backstage, fashion week backstage, behind the scenes, fashion week behind the scenes, season: 2016, series: vogue fashion week  Vogue Fashion Week - Gucci Spring 2016 Ready-to-Wear\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docMeaning(\"0_2umw3t4n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def multiplyByDiagonalMatrix(mat: RowMatrix, diag: Vector): RowMatrix = {\n",
    "#     val sArr: Array[Double] = diag.toArray\n",
    "#     new RowMatrix(mat.rows.map(vec => {\n",
    "#       val vecArr: Array[Double] = vec.toArray\n",
    "#       val newArr: Array[Double] = (0 until vec.size).toArray.map(i => vecArr(i) * sArr(i))\n",
    "#       Vectors.dense(newArr)\n",
    "#     }))\n",
    "# }\n",
    "\n",
    "# def rowsNormalized(mat: RowMatrix): RowMatrix = {\n",
    "#     new RowMatrix(mat.rows.map(vec => {\n",
    "#       val length: Double = math.sqrt(vec.toArray.map(x => x * x).sum)\n",
    "#       Vectors.dense(vec.toArray.map(_ / length))\n",
    "#     }))\n",
    "# }\n",
    "import math\n",
    "sArr = svd.s.toArray()\n",
    "def multiply(vector):\n",
    "    vecArr = vector.toArray()\n",
    "    newArr = []\n",
    "    for i in xrange(vecArr.size):\n",
    "        newArr.append(vecArr[i]*sArr[i])\n",
    "    return newArr\n",
    "\n",
    "def normalizeRows(vector):\n",
    "    vecArr = vector.toArray()\n",
    "    length = math.sqrt(sum(map(lambda x: x*x,vecArr)))\n",
    "    output = map(lambda x: x/length,vecArr)\n",
    "    return output\n",
    "            \n",
    "def multiplyByDiagonalMatrix(mat,diag):\n",
    "#     sArr = diag.toArray()\n",
    "    return RowMatrix(mat.rows.map(multiply))\n",
    "    \n",
    "def rowsNormalized(mat):\n",
    "    return RowMatrix(mat.rows.map(normalizeRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "US = multiplyByDiagonalMatrix(svd.U, svd.s)\n",
    "normalizedUS = rowsNormalized(US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docRowArr = []\n",
    "def row(mat,idd):\n",
    "    return mat.rows.zipWithUniqueId().map(lambda x: (x[1],x[0])).lookup(idd)[0].toArray()\n",
    "    \n",
    "def matMultiply(row):\n",
    "    vecArr = row.toArray()\n",
    "    newArr = []\n",
    "    summ = 0\n",
    "    for i in xrange(vecArr.size):\n",
    "        summ = summ + vecArr[i]*docRowArr[i]\n",
    "    return summ    \n",
    "\n",
    "def topDocsForDoc(normalizedUS, docId, docIds, numTopDocs):\n",
    "    global docRowArr\n",
    "    docRowArr = row(normalizedUS, docId)\n",
    "    docRowVec = Matrices.dense(len(docRowArr), 1, docRowArr)\n",
    "    docScores = normalizedUS.rows.zipWithUniqueId().map(lambda x: (x[1],x[0])).mapValues(matMultiply).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "    similarDocs = map(lambda x: docIds[x[1]],docScores.take(numTopDocs))\n",
    "    map(lambda x: docMeaning(x),similarDocs)\n",
    "    \n",
    "def printTopDocsForDoc(normalizedUS, doc, idDocs, docIds, numTopDocs):\n",
    "    if idDocs.has_key(doc):\n",
    "        docID = idDocs[doc]\n",
    "        output = topDocsForDoc(normalizedUS, docID, docIds, numTopDocs)\n",
    "    else:\n",
    "        print \"Document not found. Enter another document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_kirvzs3g\n",
      "Mark Halperin of Bloomberg News joins TODAY to talk about the growing rift in the Republican party now that Donald Trump is their likely presidential nominee. He predicts the GOP convention will be \"a big mess\" and questions whether Trump can win the White House without softening his tone.   Analyst Mark Halperin: 'Very hard' for Donald Trump to win with current tone\n",
      "\n",
      "0_m7s770cu\n",
      "Bernie Sanders beats out Clinton in Michigan primary, explosion in Seattle hurts firefighters, solar eclipse leaves some Indonesians in total darkness. Check out these top headlines on What’s Happening Today! news, current events, headlines, what’s happening today, bernie sanders, hillary clinton, democrat, primary, michigan, donald trump, election, result, seattle, explosion, firefighter, gas, solar eclipse, march 2016, viewing, indonesia, what is   Bernie Wins Michigan || Seattle explosion hurts 9 firefighters || Total solar eclipse in Pacific\n",
      "\n",
      "0_ogvv8hbh\n",
      "Ted Cruz bests Trump while Clinton and Sanders virtually tied at the Iowa caucuses. Plus, Google dethrones Apple as most valuable company, and Punxsutawney Phil emerges on Groundhogs day. Check out these stories on What’s Happening Today! news, current events, headlines, what’s happening today, iowa caucus, winner, ted cruz, donald trump, hillary clinton, bernie sanders, percent, voting, google, alphabet, apple, most valuable company, stocks, groundhog day, shadow, ceremony, pennsylvania, punxsutawney phil  Iowa Caucus results are in! So is the Groundhog’s Day prediction!\n",
      "\n",
      "0_84ebrzzn\n",
      "Meet the Press moderator Chuck Todd, NBC News political analyst Nicolle Wallace and MSNBC host Steve Kornacki discuss Donald Trump's general election prospects if there is a faction of Republicans who won't get behind his nomination.   Donald Trump is 'confident' - but can he unite the GOP?\n",
      "\n",
      "0_xbtbayg8\n",
      "NBC News political analyst Nicolle Wallace tells TODAY that Speaker Paul Ryan will have to continue to rule out a White House because the party needs him to run whether he wants to or not. She says that with the party base \"deeply suspicious\" that Donald Trump will be denied the nomination, \"there are no white horses\" for last-minute candidates to ride in on, meaning the GOP's candidate will be Donald Trump, Ted Cruz or John Kasich.   Analyst Nicolle Wallace: 'There are no white horses' to save GOP\n",
      "\n",
      "0_kbzhed5r\n",
      "Sept. 24 -- Bloomberg's Phil Mattingly runs down the results of the latest Bloomberg News poll on the race for the Republican presidential nomination. He speaks on \"Bloomberg Surveillance.\" bloomberg, politics, gop, 2016, presidential, campaign, poll  Trump Still Towers Over GOP and 'Likeable' Carson\n",
      "\n",
      "0_jtnuh434\n",
      "As the only female Republican candidate, Carly Fiorina is working to set herself apart from her competitors. So, who is Carly Fiorina? carly fiorina, republican, politics, united states, us elections, presidential elections, fiorina, hewlett packard, donald trump, republican party, hp, tech, california, john mccain, planned parenthood, planned parenthood fiorina, fiorina abortion, drug abuse, drug decriminalization, drug policy, abortion policy, us politics, marijuana, weed legalization, iran nuclear deal, lgbt, gay marriage, obamacare, testtube, test tube, news, trump 2016, fiorina 2016  Who is Carly Fiorina?\n",
      "\n",
      "0_khbc73zd\n",
      "Despite a rise in mass shootings, U.S. gun control laws have barely budged. So what do Republican candidates have to say about gun control? republicans, guns, gun laws, second amendment republicans and guns, donald trump on guns, why do republicans love guns, republicans stance on guns, gun control, ted cruz, filibuster, anti gun control, marco rubio, republican candidates on guns, carly fiorina, john kasich, marco rubio on guns, ben carson on guns, chris christie on guns, rand paul on guns, ted cruz on guns, gun policy in us, testtube, test tube, news, current events, sandy hook, san bernardino  Where Do Republican Candidates Stand On Gun Control?\n",
      "\n",
      "0_fzu2656y\n",
      "Despite a rise in mass shootings, U.S. gun control laws have barely budged. So what do Republican candidates have to say about gun control? republicans, guns, gun laws, second amendment republicans and guns, donald trump on guns, why do republicans love guns, republicans stance on guns, gun control, ted cruz, anti gun control, marco rubio, republican candidates on guns, carly fiorina, john kasich, marco rubio on guns, ben carson on guns, chris christie on guns, rand paul on guns, ted cruz on guns, gun policy in us, testtube, test tube, lissette padilla, news, current events, sandy hook, san bernardino  Where Do Republican Candidates Stand On Gun Control?\n",
      "\n",
      "0_woao0egz\n",
      "Republican frontrunner Donald Trump has withdrawn from Thursday's presidential debate with party rivals out of anger at host Fox News, leaving the last encounter before Iowa's pivotal nominating contest without the front-runner. Nathan Frandino reports. donald trump, elections, gop debate, gop iowa debate, iowa caucus, republican party, republican presidential primary debate, trump fox feud, fox news  Trump backs out of Fox News Iowa debate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printTopDocsForDoc(normalizedUS,'0_kirvzs3g', idDocs, docIds, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sArr = svd.s.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(normalizedUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# US.rows.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalizedUS.rows.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# idd = idDocs['0_kirvzs3g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print idd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rowMat = normalizedUS.rows.zipWithUniqueId().map(lambda x: (x[1],x[0])).lookup(idd)[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(rowMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rowMat.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docRowArr = row(normalizedUS,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(docRowArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(docRowArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docRowVect = Matrices.dense(len(docRowArr), 1, docRowArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(docRowVect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docRowVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(normalizedUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def matMultiply(row):\n",
    "#     vecArr = row.toArray()\n",
    "#     newArr = []\n",
    "#     summ = 0\n",
    "#     for i in xrange(vecArr.size):\n",
    "#         summ = summ + vecArr[i]*docRowArr[i]\n",
    "#     return summ    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docScores = normalizedUS.rows.zipWithUniqueId().map(lambda x: (x[1],x[0])).mapValues(matMultiply).map(lambda x: (x[1],x[0])).sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type(docScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docScores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# docScores.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similarDocs = map(lambda x: docIds[x[1]],docScores.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map(lambda x: docMeaning(x),similarDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
